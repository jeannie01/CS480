{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, PolynomialFeatures\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score\n",
    "import optuna\n",
    "import xgboost as xgb\n",
    "\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PlantDataset(Dataset):\n",
    "    def __init__(self, image_dir, df, transform=None):\n",
    "        self.image_dir = image_dir\n",
    "        self.df = df\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_id = self.df.iloc[idx, 0]\n",
    "        img_path = os.path.join(self.image_dir, f\"{img_id}.jpeg\")\n",
    "        image = Image.open(img_path)\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, img_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config:\n",
    "    def __init__(self):\n",
    "        self.seed = 32\n",
    "        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "        self.train_csv_path = 'train.csv'\n",
    "        self.test_csv_path = 'test.csv'\n",
    "        self.train_images_path = 'train_images'\n",
    "        self.test_images_path = 'test_images'\n",
    "        self.target_columns = ['X4_mean', 'X11_mean',\n",
    "                               'X18_mean', 'X50_mean', 'X26_mean', 'X3112_mean']\n",
    "        self.batch_size = 64\n",
    "\n",
    "\n",
    "def seed_everything(seed):\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "\n",
    "\n",
    "CONFIG = Config()\n",
    "seed_everything(CONFIG.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the pretrained model\n",
    "model = torch.hub.load('facebookresearch/dinov2',\n",
    "                       'dinov2_vitg14_reg').to(CONFIG.device)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform for images\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize(256, interpolation=3),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "# Load training data\n",
    "train_data = pd.read_csv(CONFIG.train_csv_path)\n",
    "test_data = pd.read_csv(CONFIG.test_csv_path)\n",
    "\n",
    "# Extract metadata and target values. From id (exclusive) to end of ancillary data\n",
    "train_metadata = train_data.iloc[:, 1:164].values\n",
    "# Get targets for training data\n",
    "train_targets = train_data[CONFIG.target_columns].values\n",
    "\n",
    "# Split the data into training and validation sets\n",
    "X_train, X_val, y_train, y_val, train_indices, val_indices = train_test_split(\n",
    "    train_metadata, train_targets, range(len(train_metadata)), test_size=0.05, random_state=42\n",
    ")\n",
    "\n",
    "# Fit and transform metadata\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_val_scaled = scaler.transform(X_val)\n",
    "test_metadata = test_data.values[:, 1:]\n",
    "test_metadata_scaled = scaler.transform(test_metadata)\n",
    "\n",
    "\n",
    "poly = PolynomialFeatures(degree=2, include_bias=False)\n",
    "# Fit and transform the training metadata using polynomial features\n",
    "X_train_poly = poly.fit_transform(X_train_scaled)\n",
    "X_val_poly = poly.transform(X_val_scaled)\n",
    "test_metadata_poly = poly.transform(test_metadata_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Datasets and DataLoaders\n",
    "train_dataset = PlantDataset(\n",
    "    image_dir='train_images', df=train_data.iloc[train_indices], transform=transform)\n",
    "val_dataset = PlantDataset(image_dir='train_images',\n",
    "                           df=train_data.iloc[val_indices], transform=transform)\n",
    "test_dataset = PlantDataset(image_dir='test_images',\n",
    "                            df=test_data, transform=transform)\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset, batch_size=CONFIG.batch_size, shuffle=False)\n",
    "val_loader = DataLoader(\n",
    "    val_dataset, batch_size=CONFIG.batch_size, shuffle=False)\n",
    "test_loader = DataLoader(\n",
    "    test_dataset, batch_size=CONFIG.batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_embeddings(loader, model):\n",
    "    embeddings = []\n",
    "    for images, _ in tqdm(loader, desc=\"Extracting embeddings\"):\n",
    "        with torch.no_grad():\n",
    "            # obtain the embeddings, put them on the cpu and convert them to a numpy array\n",
    "            batch_embeddings = model(images).cpu().numpy()\n",
    "            embeddings.append(batch_embeddings)\n",
    "    # stack all of the embeddings into one single array\n",
    "    return np.vstack(embeddings)\n",
    "\n",
    "\n",
    "suffix = 'img_embs'\n",
    "train_embeddings = extract_embeddings(train_loader, model)\n",
    "np.save(f'torch_giant_five/train_{suffix}', np.array(train_embeddings))\n",
    "val_embeddings = extract_embeddings(val_loader, model)\n",
    "np.save(f'torch_giant_five/val_{suffix}', np.array(val_embeddings))\n",
    "test_embeddings = extract_embeddings(test_loader, model)\n",
    "np.save(f'torch_giant_five/test_{suffix}', np.array(test_embeddings))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the saved extracted embeddings\n",
    "train_embeddings = np.load('torch_giant_five/train_img_embs.npy')\n",
    "val_embeddings = np.load('torch_giant_five/val_img_embs.npy')\n",
    "test_embeddings = np.load('torch_giant_five/test_img_embs.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine polynomial features with embeddings\n",
    "x_train_full_poly = np.concatenate([train_embeddings, X_train_poly], axis=1)\n",
    "x_val_poly = np.concatenate([val_embeddings, X_val_poly], axis=1)\n",
    "x_test_poly = np.concatenate([test_embeddings, test_metadata_poly], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPTIONAL: Use optuna to find the best result\n",
    "from optuna.pruners import MedianPruner\n",
    "\n",
    "\n",
    "def objective(trial):\n",
    "    xgb_params = {\n",
    "        # Expanded upper bound for n_estimators, keeping the minimum at 1500\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 1400, 3000),\n",
    "        # Expanded range for learning_rate\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.015, 0.06),\n",
    "        # Slightly expanded range for max_depth\n",
    "        'max_depth': trial.suggest_int('max_depth', 6, 12),\n",
    "        # Slightly expanded range for subsample\n",
    "        'subsample': trial.suggest_float('subsample', 0.65, 0.95),\n",
    "        # Expanded range for colsample_bytree\n",
    "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.65, 0.95),\n",
    "        # Expanded range for reg_alpha\n",
    "        'reg_alpha': trial.suggest_float('reg_alpha', 0.15, 0.85),\n",
    "        # Expanded range for reg_lambda\n",
    "        'reg_lambda': trial.suggest_float('reg_lambda', 1.0, 6.5),\n",
    "        # Adjusted range for min_child_weight\n",
    "        'min_child_weight': trial.suggest_int('min_child_weight', 2, 8),\n",
    "        'objective': 'reg:squarederror',\n",
    "        'eval_metric': 'rmse',\n",
    "        'random_state': CONFIG.seed\n",
    "    }\n",
    "\n",
    "    # Train and evaluate the model\n",
    "    r2_scores = {}\n",
    "    for i, trait in tqdm(enumerate(CONFIG.target_columns), total=len(CONFIG.target_columns)):\n",
    "        model = xgb.XGBRegressor(**xgb_params)\n",
    "\n",
    "        eval_set = [(x_train_full_poly, y_train[:, i]),\n",
    "                    (x_val_poly, y_val[:, i])]\n",
    "        model.fit(\n",
    "            x_train_full_poly,\n",
    "            y_train[:, i],\n",
    "            eval_set=eval_set,\n",
    "            verbose=False  # Disable training logs\n",
    "        )\n",
    "\n",
    "        # Prune the trial if it's not improving\n",
    "        val_predictions = model.predict(x_val_poly)\n",
    "        r2_scores[trait] = r2_score(y_val[:, i], val_predictions)\n",
    "\n",
    "        # Report the intermediate R2 score\n",
    "        trial.report(-r2_scores[trait], i)\n",
    "\n",
    "        # Prune the trial if it is getting worse\n",
    "        if trial.should_prune():\n",
    "            print(f'Trial {trial.number} pruned at column {i}')\n",
    "            raise optuna.TrialPruned()\n",
    "\n",
    "    # Return the negative average R2 score\n",
    "    average_r2_val = sum(r2_scores.values()) / len(r2_scores)\n",
    "    return -average_r2_val\n",
    "\n",
    "\n",
    "study = optuna.create_study(direction='minimize', pruner=MedianPruner())\n",
    "\n",
    "# Optimize the objective function\n",
    "study.optimize(objective, n_trials=50)\n",
    "\n",
    "fig = optuna.visualization.plot_optimization_history(study)\n",
    "fig.show()\n",
    "\n",
    "# Get the best trial\n",
    "best_trial = study.best_trial\n",
    "print(f'Best trial: {best_trial.number}')\n",
    "print(f'Best value: {-best_trial.value}')\n",
    "print(f'Best params: {best_trial.params}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print best hyperparameters\n",
    "print(\"Best hyperparameters:\", study.best_params)\n",
    "\n",
    "# Train your final model with the best parameters found from optuna\n",
    "best_xgb_params = study.best_params\n",
    "best_xgb_params['objective'] = 'reg:squarederror'\n",
    "best_xgb_params['eval_metric'] = 'rmse'\n",
    "best_xgb_params['random_state'] = CONFIG.seed\n",
    "\n",
    "best_xgb_params = {\n",
    "    'n_estimators': 2906,\n",
    "    'learning_rate': 0.028283636339476847,\n",
    "    'max_depth': 7,\n",
    "    'subsample': 0.9209937477043662,\n",
    "    'colsample_bytree': 0.8045338627186344,\n",
    "    'reg_alpha': 0.3276660546210266,\n",
    "    'reg_lambda': 5.834782742225015,\n",
    "    'min_child_weight': 4,\n",
    "    'objective': 'reg:squarederror',\n",
    "    'eval_metric': 'rmse',\n",
    "    'random_state': CONFIG.seed\n",
    "}\n",
    "\n",
    "models = {}\n",
    "val_predictions = {}\n",
    "r2_scores = {}\n",
    "\n",
    "# Train, evaluate, and retrain the model\n",
    "for i, trait in tqdm(enumerate(CONFIG.target_columns), total=len(CONFIG.target_columns), desc=\"Training\"):\n",
    "    # Initialize and train the model\n",
    "    models[trait] = xgb.XGBRegressor(**best_xgb_params)\n",
    "    models[trait].fit(x_train_full_poly, y_train[:, i])\n",
    "\n",
    "    # Evaluate on the validation set\n",
    "    val_predictions[trait] = models[trait].predict(x_val_poly)\n",
    "    r2_scores[trait] = r2_score(y_val[:, i], val_predictions[trait])\n",
    "    print(f'R2 score for {trait} on validation set: {r2_scores[trait]}')\n",
    "\n",
    "print(\"----------FINISHING TRAINING----------\")\n",
    "# Calculate the average R2 score on the validation set\n",
    "average_r2_val = sum(r2_scores.values()) / len(r2_scores)\n",
    "print(f'Average R2 score on validation set: {average_r2_val}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test predictions with polynomial features\n",
    "test_predictions = {}\n",
    "for i, trait in tqdm(enumerate(CONFIG.target_columns), total=len(CONFIG.target_columns)):\n",
    "    # Make predictions for the test set using the model trained on polynomial features\n",
    "    test_predictions[trait] = models[trait].predict(x_test_poly)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame for the submission\n",
    "submission_df = pd.DataFrame({\n",
    "    'id': test_data['id'],\n",
    "    'X4': test_predictions['X4_mean'],\n",
    "    'X11': test_predictions['X11_mean'],\n",
    "    'X18': test_predictions['X18_mean'],\n",
    "    'X26': test_predictions['X26_mean'],\n",
    "    'X50': test_predictions['X50_mean'],\n",
    "    'X3112': test_predictions['X3112_mean'],\n",
    "})\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "submission_df.to_csv(f'v25.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
